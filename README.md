# Bias Detection in AI-Generated Text

An automated system for detecting and analyzing bias in text generated by language models. This project detects six types of bias: gender, race, religion, political, socioeconomic, and age.

## Overview

This system helps identify potential bias in AI-generated text through pattern matching and lexicon-based detection. It provides a web interface where users can analyze text, see highlighted biased terms, and get recommendations for improvement.

The system uses curated lexicons and pattern detection to identify bias across multiple categories with severity scoring (mild, moderate, severe).

## Getting Started

### Prerequisites

- Python 3.8+
- Node.js 18+
- pip and npm

### Installation

1. **Clone the repository**
```bash
git clone <repository-url>
cd bias-detection-system
```

2. **Backend Setup**
```bash
cd backend
python -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate
pip install -r requirements.txt
python -c "import nltk; nltk.download('punkt')"
```

3. **Frontend Setup**
```bash
cd ../frontend
npm install
```

### Running the Application

1. **Start Backend** (Terminal 1)
```bash
cd backend
source venv/bin/activate
uvicorn app.main:app --reload --host 0.0.0.0 --port 8000
```

2. **Start Frontend** (Terminal 2)
```bash
cd frontend
npm run dev
```

3. **Access the Application**
- Frontend: http://localhost:5173
- Backend API Docs: http://localhost:8000/docs

## Features

- Detects 6 bias categories: gender, race, religion, political, socioeconomic, age
- Real-time text analysis with highlighting
- Severity scoring (mild, moderate, severe)
- Export results as JSON or text reports
- Category filtering
- Comprehensive analysis mode with recommendations

## How It Works

The system uses lexicon-based detection with pattern matching. Each bias category has curated lists of terms and patterns that indicate potential bias. When text is analyzed:

1. Text is checked against bias lexicons
2. Patterns are matched using regex with context windows
3. Severity is calculated based on frequency and pattern strength
4. Biased terms are highlighted in the original text
5. Recommendations are generated based on detected bias types

## Dataset

We use the BEADs (Bias Evaluation Across Domains) dataset from Hugging Face, which contains over 3.6 million annotated examples across multiple bias categories. The dataset includes labels for bias type, severity, sentiment, and toxicity.

## Team

| Team Member | Role | Responsibilities |
|-------------|------|------------------|
| **Abdula Ameen** | Backend Lead | API development, model integration, dataset processing, evaluation pipelines |
| **Ali Zahr** | Frontend Lead | UI/UX design, React components, frontend-backend integration |
| **Chance Inosencio** | ML Engineer | Model training, fine-tuning, evaluation metrics, bias analysis |
| **Ghina Albabbili** | Documentation | Written report, visualizations, presentation, analysis coordination |

## API Usage

### Detect Bias
```bash
curl -X POST "http://localhost:8000/api/v1/detect" \
  -H "Content-Type: application/json" \
  -d '{"text": "The female nurse assisted the male doctor."}'
```

### Comprehensive Analysis
```bash
curl -X POST "http://localhost:8000/api/v1/analyze" \
  -H "Content-Type: application/json" \
  -d '{"text": "Your text here"}'
```

## Bias Categories

**Gender** - Occupation stereotypes, gendered trait associations, pronoun bias

**Race** - Stereotypical associations, coded language, discriminatory patterns

**Religion** - Extremism associations, prejudicial terms, stereotyping

**Political** - Partisan language, inflammatory rhetoric, loaded terms

**Socioeconomic** - Class-based stereotypes, wealth assumptions, status bias

**Age** - Ageism, generational stereotypes, workplace discrimination

## Testing

Run the automated test suite:
```bash
cd backend
pytest
```

## Documentation

- [Backend API Documentation](./backend/README.md)
- [Frontend Documentation](./frontend/README.md)
- [API Reference](http://localhost:8000/docs) (when running)

## References

- BEADs Dataset: https://huggingface.co/datasets/shainar/BEAD
- StereoSet: https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00355/96428
- Jigsaw Toxicity: https://www.kaggle.com/c/jigsaw-unintended-bias-in-toxicity-classification

## Project Information

**Institution:** University of Michigan - Dearborn
**Course:** CIS 411
**Semester:** Fall 2025

**Team:** Abdula Ameen, Ali Zahr, Chance Inosencio, Ghina Albabbili
